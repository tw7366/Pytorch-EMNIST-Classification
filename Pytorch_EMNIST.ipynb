{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* Instead of building a simple model, try creating a more advanced CNN model that can be easily tuned\n",
    "* Create models with varying depth (number of convolutional layers) to observe performance vs time consumption\n",
    "* Apply various hyperparameter tuning techniques to CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import Normalize, ToTensor\n",
    "import torch.nn as nn  # neural network\n",
    "import torch.optim as optim  # optimization layer\n",
    "import torch.nn.functional as F  # activation functions\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 90240, validation set size: 22560\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use gpu if available\n",
    "\n",
    "# load data in\n",
    "train_set = datasets.EMNIST(root=\"data\", split=\"balanced\",\n",
    "                            train=True, transform=transforms.Compose([ToTensor()])\n",
    "                           )\n",
    "test_set = datasets.EMNIST(root=\"data\", split=\"balanced\", \n",
    "                           train=False,transform=transforms.Compose([ToTensor()])\n",
    "                          )\n",
    "entire_trainset = torch.utils.data.DataLoader(train_set, shuffle=True)\n",
    "\n",
    "split_train_size = int(0.8*(len(entire_trainset)))  # use 80% as train set\n",
    "split_valid_size = len(entire_trainset) - split_train_size  # use 20% as validation set\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(train_set, [split_train_size, split_valid_size]) \n",
    "\n",
    "print(f'train set size: {split_train_size}, validation set size: {split_valid_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the 4 arbitrary models to evaluate\n",
    "model_codes = {\n",
    "    'model_1': [64, 'M', 128, 'M', 'D', 256, 'M', 512, 'M', 'D'],\n",
    "    'model_2': [64, 'M', 128, 'M', 'D', 256, 256, 'M', 512, 512, 'M', 'D'],\n",
    "    'model_3': [64, 64, 'M', 128, 128, 'M', 'D', 256, 256, 256, 'M', 512, 512, 512, 'M', 'D'],\n",
    "    'model_4': [64, 64, 64, 64, 'M', 128, 128, 128, 128, 'M', 'D', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 'D']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple CNN model like below, you have to add layers manually, which is a problem when creating a larger neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 3 convolutional layers\n",
    "        self.cv1 = nn.Conv2d(in_channel=1,out_channels=16,kernel_size=5, stride=1)  # input: 1 if grayscale, 3 if RGB\n",
    "        self.cv2 = nn.Conv2d(16, 64, 5)\n",
    "        self.cv3 = nn.Conv2d(64, 128, 5)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Dense layer - (fully connected)\n",
    "        self.fc1 = nn.Linear(in_features=128*3*3, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=47)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        forward method explicitly defines the network's transformation.\n",
    "        forward method maps an input tensor to a prediction output tensor\n",
    "        '''\n",
    "        # hidden convolutional layers\n",
    "        x = F.relu(self.cv1(x))\n",
    "        x = F.relu(self.cv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.cv3(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # hidden linear layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = x.view(-1, 128*3*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # output layer\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a function inside the CNN class to automatically create convolutional layers given a list of inputs of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easily tunable model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, model_code, in_channels, out_dim, act, use_bn, dropout):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        if act == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif act == 'leakyrelu':\n",
    "            self.act = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(\"Not a valid activation function\")\n",
    "            \n",
    "        \n",
    "        self.layers = self.make_layers(model_code, in_channels, use_bn, dropout)\n",
    "        self.classifier = nn.Sequential(nn.Linear(512, 256),\n",
    "                                        self.act,\n",
    "                                        nn.Linear(256, out_dim)\n",
    "                                       )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        # skipped softmax siince cross-entropy loss is used\n",
    "        return x\n",
    "    \n",
    "    def make_layers(self, model_code, in_channels, use_bn, dropout):\n",
    "        layers = []\n",
    "        for x in model_codes[model_code]:\n",
    "            if x == \"M\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif x == 'D':\n",
    "                layers += [nn.Dropout(dropout)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels=in_channels,\n",
    "                                    out_channels=x,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=1,\n",
    "                                    padding=1)]\n",
    "                if use_bn:\n",
    "                    layers += [nn.BatchNorm2d(x)]\n",
    "                layers += [self.act]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate, Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, criterion, args):\n",
    "    '''\n",
    "    Returns validation loss and accuracy\n",
    "    \n",
    "        Parameters:\n",
    "            net (CNN): a convolutional neural network to train\n",
    "            optimizer: optimizer\n",
    "            criterion (loss function): a loss function to evaluate the model on\n",
    "            args (ArgumentParser): hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            net (CNN): a trained model\n",
    "            train_loss (float): train loss\n",
    "            train_acc (float): train accuracy\n",
    "    '''\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.train_batch, shuffle=True)\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # the class with the highest value is the prediction\n",
    "        _, prediction = torch.max(outputs.data, 1)  # grab prediction as one-dimensional tensor\n",
    "        total += labels.size(0)\n",
    "        correct += (prediction == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    return net, train_loss, train_acc  # net is returned to be fed to the test function later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, criterion, args):\n",
    "    '''\n",
    "    Returns validation loss and accuracy\n",
    "    \n",
    "        Parameters:\n",
    "            net (CNN): a convolutional neural network to validate\n",
    "            criterion (loss function): a loss function to evaluate the model on\n",
    "            args (ArgumentParser): hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            val_loss (float): validation loss\n",
    "            val_acc (float): validation accuracy\n",
    "    '''\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=args.test_batch, shuffle=True)\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (prediction == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, args):\n",
    "    '''\n",
    "    Returns test accuracy\n",
    "    \n",
    "        Parameters:\n",
    "            net (CNN): a trained model\n",
    "            args (ArgumentParser): hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            test_acc (float): test accuracy of a trained model\n",
    "    '''\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.test_batch, shuffle=True)\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(args):\n",
    "    '''\n",
    "    Execute train and validate functions epoch-times to train a CNN model.\n",
    "    Each time, store train & validation loss and accuracy.\n",
    "    Then, test the model and return the result.\n",
    "    \n",
    "        Parameter:\n",
    "            args (ArgumentParser): hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            vars(args) (Dictionary): settings of the model\n",
    "            results (OrderedDict): stored stats of each epoch + test accuracy\n",
    "    '''\n",
    "    net = CNN(model_code = args.model_code,\n",
    "              in_channels = args.in_channels, \n",
    "              out_dim = args.out_dim, \n",
    "              act = args.act, \n",
    "              use_bn = args.use_bn, \n",
    "              dropout = args.dropout\n",
    "             )\n",
    "    net = net.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # select an optimizer\n",
    "    if args.optim == 'adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr)  # learning rate\n",
    "    elif args.optim == 'sgd':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer')\n",
    "\n",
    "    # containers to keep track of statistics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    time_total = 0\n",
    "        \n",
    "    for epoch in range(args.epoch):  # number of training to be completed\n",
    "        time_start = time.time()\n",
    "        net, train_loss, train_acc = train(net, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, criterion, args)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        time_duration = round(time_end - time_start, 2)\n",
    "        time_total += time_duration\n",
    "        \n",
    "        # print results of each iteration\n",
    "        print(f'Epoch {epoch+1}, Accuracy(train, validation):{round(train_acc, 2), round(val_acc, 2)}, '\n",
    "              f'Loss(train, validation):{round(train_loss, 4), round(val_loss, 4)}, Time: {time_duration}s')\n",
    "\n",
    "    test_acc = test(net, args)\n",
    "\n",
    "    results = OrderedDict()\n",
    "    results['train_losses'] = [round(x, 4) for x in train_losses]\n",
    "    results['val_losses'] = [round(x, 4) for x in val_losses]\n",
    "    results['train_accs'] = [round(x, 2) for x in train_accs]\n",
    "    results['val_accs'] = [round(x, 2) for x in val_accs]\n",
    "    results['train_acc'] = round(train_acc, 2)\n",
    "    results['val_acc'] = round(val_acc, 2)\n",
    "    results['test_acc'] = round(test_acc, 2)\n",
    "    results['time_total'] = round(time_total, 2)\n",
    "    \n",
    "    return vars(args), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(act='relu', dropout=0.3, epoch=10, in_channels=1, lr=0.001, model_code='model_2', optim='sgd', out_dim=47, test_batch=256, train_batch=256, use_bn=True)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#### Model Capacity ####\n",
    "args.model_code = 'model_2'\n",
    "args.in_channels = 1\n",
    "args.out_dim = 47\n",
    "args.act = 'relu'\n",
    "\n",
    "#### Regularization ####\n",
    "args.dropout = 0.3\n",
    "args.use_bn = True\n",
    "\n",
    "#### Optimization ####\n",
    "args.optim = 'sgd'\n",
    "args.lr = 0.001  # learning rate\n",
    "args.epoch = 10\n",
    "args.train_batch = 256\n",
    "args.test_batch = 256\n",
    "\n",
    "#### Experimental Variables ####\n",
    "models = ['model_1', 'model_2', 'model_3', 'model_4']\n",
    "optims = ['adam', 'sgd']\n",
    "split_sizes = [0.7, 0.8, 0.85]\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be focusing on the impact of structures and optimizers to a model first as I believe they're the hyperparmeters with the\n",
    "biggest influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model code: model_1 with adam optimizer\n",
      "Epoch 1, Accuracy(train, validation):(77.85, 86.13), Loss(train, validation):(0.7184, 0.3928), Time: 17.61s\n",
      "Epoch 2, Accuracy(train, validation):(86.2, 87.79), Loss(train, validation):(0.3838, 0.3439), Time: 17.32s\n",
      "Epoch 3, Accuracy(train, validation):(87.62, 88.43), Loss(train, validation):(0.3412, 0.3234), Time: 17.51s\n",
      "Epoch 4, Accuracy(train, validation):(88.46, 88.59), Loss(train, validation):(0.314, 0.3221), Time: 17.33s\n",
      "Epoch 5, Accuracy(train, validation):(88.93, 88.98), Loss(train, validation):(0.2927, 0.3087), Time: 16.99s\n",
      "Epoch 6, Accuracy(train, validation):(89.36, 89.24), Loss(train, validation):(0.277, 0.3041), Time: 16.95s\n",
      "Epoch 7, Accuracy(train, validation):(89.97, 89.19), Loss(train, validation):(0.2607, 0.307), Time: 16.34s\n",
      "Epoch 8, Accuracy(train, validation):(90.24, 89.26), Loss(train, validation):(0.2506, 0.3106), Time: 16.8s\n",
      "Epoch 9, Accuracy(train, validation):(90.6, 89.25), Loss(train, validation):(0.2406, 0.3034), Time: 17.02s\n",
      "Epoch 10, Accuracy(train, validation):(90.93, 89.74), Loss(train, validation):(0.2298, 0.3007), Time: 17.18s\n",
      "{'model_code': 'model_1', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'adam', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 89.31\n",
      "Total time duration: 171.05\n",
      "\n",
      "model code: model_1 with sgd optimizer\n",
      "Epoch 1, Accuracy(train, validation):(34.37, 70.12), Loss(train, validation):(2.7555, 1.325), Time: 17.05s\n",
      "Epoch 2, Accuracy(train, validation):(72.23, 80.87), Loss(train, validation):(1.0128, 0.6717), Time: 16.8s\n",
      "Epoch 3, Accuracy(train, validation):(78.71, 82.97), Loss(train, validation):(0.6914, 0.5403), Time: 16.73s\n",
      "Epoch 4, Accuracy(train, validation):(81.25, 84.52), Loss(train, validation):(0.5797, 0.4788), Time: 16.75s\n",
      "Epoch 5, Accuracy(train, validation):(82.88, 85.94), Loss(train, validation):(0.5229, 0.4318), Time: 16.71s\n",
      "Epoch 6, Accuracy(train, validation):(83.89, 86.06), Loss(train, validation):(0.4839, 0.4137), Time: 17.03s\n",
      "Epoch 7, Accuracy(train, validation):(84.57, 86.72), Loss(train, validation):(0.4551, 0.3926), Time: 16.75s\n",
      "Epoch 8, Accuracy(train, validation):(85.21, 87.09), Loss(train, validation):(0.4339, 0.38), Time: 16.69s\n",
      "Epoch 9, Accuracy(train, validation):(85.63, 86.92), Loss(train, validation):(0.4195, 0.3741), Time: 16.7s\n",
      "Epoch 10, Accuracy(train, validation):(86.05, 87.4), Loss(train, validation):(0.4056, 0.3646), Time: 16.7s\n",
      "{'model_code': 'model_1', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'sgd', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 86.65\n",
      "Total time duration: 167.91\n",
      "\n",
      "model code: model_2 with adam optimizer\n",
      "Epoch 1, Accuracy(train, validation):(78.93, 86.54), Loss(train, validation):(0.6529, 0.3761), Time: 21.12s\n",
      "Epoch 2, Accuracy(train, validation):(86.79, 88.16), Loss(train, validation):(0.36, 0.3317), Time: 21.0s\n",
      "Epoch 3, Accuracy(train, validation):(88.24, 88.66), Loss(train, validation):(0.3157, 0.3233), Time: 20.98s\n",
      "Epoch 4, Accuracy(train, validation):(88.91, 88.57), Loss(train, validation):(0.2925, 0.3109), Time: 20.99s\n",
      "Epoch 5, Accuracy(train, validation):(89.47, 89.09), Loss(train, validation):(0.2747, 0.317), Time: 20.98s\n",
      "Epoch 6, Accuracy(train, validation):(89.92, 88.93), Loss(train, validation):(0.2616, 0.3165), Time: 20.96s\n",
      "Epoch 7, Accuracy(train, validation):(90.35, 89.42), Loss(train, validation):(0.247, 0.3), Time: 21.02s\n",
      "Epoch 8, Accuracy(train, validation):(90.67, 89.34), Loss(train, validation):(0.2356, 0.2997), Time: 21.0s\n",
      "Epoch 9, Accuracy(train, validation):(91.14, 88.99), Loss(train, validation):(0.2236, 0.3178), Time: 21.04s\n",
      "Epoch 10, Accuracy(train, validation):(91.33, 89.54), Loss(train, validation):(0.2153, 0.3002), Time: 21.03s\n",
      "{'model_code': 'model_2', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'adam', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 89.24\n",
      "Total time duration: 210.12\n",
      "\n",
      "model code: model_2 with sgd optimizer\n",
      "Epoch 1, Accuracy(train, validation):(51.15, 79.58), Loss(train, validation):(1.9595, 0.6796), Time: 20.57s\n",
      "Epoch 2, Accuracy(train, validation):(80.44, 84.69), Loss(train, validation):(0.6245, 0.4575), Time: 20.51s\n",
      "Epoch 3, Accuracy(train, validation):(83.82, 86.46), Loss(train, validation):(0.4848, 0.3922), Time: 20.56s\n",
      "Epoch 4, Accuracy(train, validation):(85.49, 87.03), Loss(train, validation):(0.4256, 0.3638), Time: 20.53s\n",
      "Epoch 5, Accuracy(train, validation):(86.33, 87.55), Loss(train, validation):(0.3929, 0.3467), Time: 20.52s\n",
      "Epoch 6, Accuracy(train, validation):(87.02, 88.26), Loss(train, validation):(0.3691, 0.331), Time: 20.53s\n",
      "Epoch 7, Accuracy(train, validation):(87.62, 88.52), Loss(train, validation):(0.3513, 0.3217), Time: 20.61s\n",
      "Epoch 8, Accuracy(train, validation):(88.01, 88.61), Loss(train, validation):(0.3367, 0.3248), Time: 20.71s\n",
      "Epoch 9, Accuracy(train, validation):(88.27, 88.28), Loss(train, validation):(0.3257, 0.315), Time: 20.58s\n",
      "Epoch 10, Accuracy(train, validation):(88.66, 88.99), Loss(train, validation):(0.3138, 0.3098), Time: 20.56s\n",
      "{'model_code': 'model_2', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'sgd', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 88.33\n",
      "Total time duration: 205.68\n",
      "\n",
      "model code: model_3 with adam optimizer\n",
      "Epoch 1, Accuracy(train, validation):(75.87, 85.89), Loss(train, validation):(0.7486, 0.4018), Time: 31.81s\n",
      "Epoch 2, Accuracy(train, validation):(86.72, 85.47), Loss(train, validation):(0.3656, 0.3972), Time: 31.9s\n",
      "Epoch 3, Accuracy(train, validation):(87.94, 88.06), Loss(train, validation):(0.3257, 0.3282), Time: 31.8s\n",
      "Epoch 4, Accuracy(train, validation):(88.83, 88.03), Loss(train, validation):(0.2992, 0.3267), Time: 31.79s\n",
      "Epoch 5, Accuracy(train, validation):(89.32, 89.03), Loss(train, validation):(0.2823, 0.323), Time: 31.74s\n",
      "Epoch 6, Accuracy(train, validation):(89.73, 89.06), Loss(train, validation):(0.268, 0.3103), Time: 31.67s\n",
      "Epoch 7, Accuracy(train, validation):(90.11, 89.45), Loss(train, validation):(0.2563, 0.2989), Time: 31.71s\n",
      "Epoch 8, Accuracy(train, validation):(90.45, 89.59), Loss(train, validation):(0.2463, 0.2947), Time: 31.76s\n",
      "Epoch 9, Accuracy(train, validation):(90.67, 89.4), Loss(train, validation):(0.238, 0.3028), Time: 31.36s\n",
      "Epoch 10, Accuracy(train, validation):(90.96, 89.28), Loss(train, validation):(0.2289, 0.3138), Time: 31.49s\n",
      "{'model_code': 'model_3', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'adam', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 88.97\n",
      "Total time duration: 317.03\n",
      "\n",
      "model code: model_3 with sgd optimizer\n",
      "Epoch 1, Accuracy(train, validation):(57.28, 83.54), Loss(train, validation):(1.6857, 0.5137), Time: 31.35s\n",
      "Epoch 2, Accuracy(train, validation):(83.74, 86.43), Loss(train, validation):(0.4922, 0.392), Time: 31.52s\n",
      "Epoch 3, Accuracy(train, validation):(86.03, 87.47), Loss(train, validation):(0.3985, 0.3455), Time: 30.86s\n",
      "Epoch 4, Accuracy(train, validation):(87.24, 88.44), Loss(train, validation):(0.3574, 0.3259), Time: 29.76s\n",
      "Epoch 5, Accuracy(train, validation):(87.99, 88.48), Loss(train, validation):(0.3321, 0.3195), Time: 29.76s\n",
      "Epoch 6, Accuracy(train, validation):(88.58, 88.7), Loss(train, validation):(0.3122, 0.3124), Time: 30.06s\n",
      "Epoch 7, Accuracy(train, validation):(89.02, 88.98), Loss(train, validation):(0.2975, 0.2985), Time: 30.68s\n",
      "Epoch 8, Accuracy(train, validation):(89.35, 89.3), Loss(train, validation):(0.2843, 0.2933), Time: 30.96s\n",
      "Epoch 9, Accuracy(train, validation):(89.77, 89.44), Loss(train, validation):(0.2739, 0.2938), Time: 31.01s\n",
      "Epoch 10, Accuracy(train, validation):(89.94, 89.67), Loss(train, validation):(0.2653, 0.2855), Time: 31.39s\n",
      "{'model_code': 'model_3', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'sgd', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 89.12\n",
      "Total time duration: 307.35\n",
      "\n",
      "model code: model_4 with adam optimizer\n",
      "Epoch 1, Accuracy(train, validation):(63.67, 83.02), Loss(train, validation):(1.1305, 0.4844), Time: 49.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Accuracy(train, validation):(84.91, 85.61), Loss(train, validation):(0.4275, 0.4107), Time: 47.66s\n",
      "Epoch 3, Accuracy(train, validation):(86.62, 85.31), Loss(train, validation):(0.3697, 0.4221), Time: 46.98s\n",
      "Epoch 4, Accuracy(train, validation):(87.53, 86.36), Loss(train, validation):(0.3383, 0.3905), Time: 47.57s\n",
      "Epoch 5, Accuracy(train, validation):(88.49, 87.05), Loss(train, validation):(0.314, 0.3595), Time: 49.51s\n",
      "Epoch 6, Accuracy(train, validation):(88.8, 88.76), Loss(train, validation):(0.3005, 0.3287), Time: 49.11s\n",
      "Epoch 7, Accuracy(train, validation):(89.41, 88.79), Loss(train, validation):(0.286, 0.3195), Time: 49.29s\n",
      "Epoch 8, Accuracy(train, validation):(89.47, 88.45), Loss(train, validation):(0.2776, 0.3245), Time: 49.03s\n",
      "Epoch 9, Accuracy(train, validation):(89.72, 88.84), Loss(train, validation):(0.2684, 0.3123), Time: 48.98s\n",
      "Epoch 10, Accuracy(train, validation):(89.98, 88.93), Loss(train, validation):(0.2609, 0.3175), Time: 48.98s\n",
      "{'model_code': 'model_4', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'adam', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 88.49\n",
      "Total time duration: 486.41\n",
      "\n",
      "model code: model_4 with sgd optimizer\n",
      "Epoch 1, Accuracy(train, validation):(55.28, 84.07), Loss(train, validation):(1.6976, 0.475), Time: 48.23s\n",
      "Epoch 2, Accuracy(train, validation):(84.76, 87.15), Loss(train, validation):(0.4514, 0.3676), Time: 48.3s\n",
      "Epoch 3, Accuracy(train, validation):(87.02, 88.03), Loss(train, validation):(0.3678, 0.3329), Time: 48.37s\n",
      "Epoch 4, Accuracy(train, validation):(87.95, 87.89), Loss(train, validation):(0.3298, 0.3407), Time: 48.16s\n",
      "Epoch 5, Accuracy(train, validation):(88.73, 88.75), Loss(train, validation):(0.3055, 0.3113), Time: 47.97s\n",
      "Epoch 6, Accuracy(train, validation):(89.22, 88.92), Loss(train, validation):(0.2868, 0.2955), Time: 49.58s\n",
      "Epoch 7, Accuracy(train, validation):(89.83, 89.06), Loss(train, validation):(0.2708, 0.2977), Time: 48.46s\n",
      "Epoch 8, Accuracy(train, validation):(90.13, 89.65), Loss(train, validation):(0.2602, 0.2935), Time: 48.29s\n",
      "Epoch 9, Accuracy(train, validation):(90.4, 89.33), Loss(train, validation):(0.2515, 0.2917), Time: 48.53s\n",
      "Epoch 10, Accuracy(train, validation):(90.66, 89.41), Loss(train, validation):(0.2424, 0.2866), Time: 48.66s\n",
      "{'model_code': 'model_4', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'sgd', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 89.14\n",
      "Total time duration: 484.55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for opt in optims:\n",
    "        print(f'model code: {model} with {opt} optimizer')\n",
    "        args.model_code = model\n",
    "        args.optim = opt\n",
    "        setting, results = experiment(args)\n",
    "        print(setting)\n",
    "        print('Test Accuracy: {}'.format(results['test_acc']))\n",
    "        print('Total time duration: {}'.format(results['time_total']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "As neural nets became more complex with more nodes:\n",
    "   * execution time got extended significantly\n",
    "   * surprisingly, train loss and validation loss did not change much with the adam optimizer\n",
    "   * train loss and validation loss improved gradually with the SGD optimizer\n",
    "   \n",
    "Side Notes\n",
    "   * adam optimizer had tendency to overfit data in all models\n",
    "   * building a model is finding the right balance between performance and efficiency\n",
    "   \n",
    "   \n",
    "## Conclusion\n",
    "After certain number of convolutional layers, the difference in performance between models is quite insignificant; however, the execution time tends to always increase with the depth, causing unwanted inefficiency. Therefore, the complexity of a convolutional neural network does not justify how efficient it is. \n",
    "\n",
    "In my opinion, a combination of random searching and grid searching to find the optimal number of convolutional layers would be a good approach in reaching the ideal balance between performance and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
