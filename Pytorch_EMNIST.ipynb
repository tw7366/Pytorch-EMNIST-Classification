{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* Instead of building a simple model, try creating a more advanced CNN model that can be easily tuned\n",
    "* Create models with varying depth (number of convolutional layers) to observe performance vs time consumption\n",
    "* Apply various hyperparameter tuning techniques to CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import Normalize, ToTensor\n",
    "import torch.nn as nn  # neural network\n",
    "import torch.optim as optim  # optimization layer\n",
    "import torch.nn.functional as F  # activation functions\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use gpu if available\n",
    "\n",
    "# load data in\n",
    "train_set = datasets.EMNIST(root=\"data\", split=\"balanced\",\n",
    "                        train=True, download=True,\n",
    "                        transform=transforms.Compose([ToTensor()])\n",
    "                           )\n",
    "test_set = datasets.EMNIST(root=\"data\", split=\"balanced\", \n",
    "                       train=False, download=True, \n",
    "                       transform=transforms.Compose([ToTensor()])\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_codes = {\n",
    "    'model_1': [64, 'M', 128, 'M', 'D', 256, 'M', 512, 'M', 'D'],\n",
    "    'model_2': [64, 64, 'M', 128, 128, 'M', 'D', 256, 256, 'M', 512, 512, 'M', 'D'],\n",
    "    'model_3': [64, 64, 64, 'M', 128, 128, 128, 'M', 'D', 256, 256, 256, 'M', 512, 512, 512, 'M', 'D']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple CNN model like below, you have to add layers manually, which is a problem when creating a larger neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 3 convolutional layers\n",
    "        self.cv1 = nn.Conv2d(in_channel=1,out_channels=16,kernel_size=5, stride=1)  # input: 1 if grayscale, 3 if RGB\n",
    "        self.cv2 = nn.Conv2d(16, 64, 5)\n",
    "        self.cv3 = nn.Conv2d(64, 128, 5)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Dense layer - (fully connected)\n",
    "        self.fc1 = nn.Linear(in_features=128*3*3, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=47)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        forward method explicitly defines the network's transformation.\n",
    "        forward method maps an input tensor to a prediction output tensor\n",
    "        '''\n",
    "        # hidden convolutional layers\n",
    "        x = F.relu(self.cv1(x))\n",
    "        x = F.relu(self.cv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.cv3(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # hidden linear layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = x.view(-1, 128*3*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # output layer\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a function inside the CNN class to automatically create convolutional layers given a list of inputs of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easily tunable model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, model_code, in_channels, out_dim, act, use_bn, dropout):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        if act == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif act == 'leakyrelu':\n",
    "            self.act = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(\"Not a valid activation function\")\n",
    "            \n",
    "        \n",
    "        self.layers = self.make_layers(model_code, in_channels, use_bn, dropout)\n",
    "        self.classifier = nn.Sequential(nn.Linear(512, 256),\n",
    "                                        self.act,\n",
    "                                        nn.Linear(256, out_dim)\n",
    "                                       )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        # skipped softmax siince cross-entropy loss is used\n",
    "        return x\n",
    "    \n",
    "    def make_layers(self, model_code, in_channels, use_bn, dropout):\n",
    "        layers = []\n",
    "        for x in model_codes[model_code]:\n",
    "            if x == \"M\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif x == 'D':\n",
    "                layers += [nn.Dropout(dropout)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels=in_channels,\n",
    "                                    out_channels=x,\n",
    "                                    kernel_size=3,\n",
    "                                    stride=1,\n",
    "                                    padding=1)]\n",
    "                if use_bn:\n",
    "                    layers += [nn.BatchNorm2d(x)]\n",
    "                layers += [self.act]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate, Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, criterion, args):\n",
    "    '''\n",
    "    Returns validation loss and accuracy\n",
    "    \n",
    "        Parameters:\n",
    "            net (CNN): a convolutional neural network to train\n",
    "            optimizer: optimizer\n",
    "            criterion (loss function): a loss function to evaluate the model on\n",
    "            args (ArgumentParser): hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            net (CNN): a trained model\n",
    "            train_loss (float): train loss\n",
    "            train_acc (float): train accuracy\n",
    "    '''\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.train_batch, shuffle=True)\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # the class with the highest value is the prediction\n",
    "        _, prediction = torch.max(outputs.data, 1)  # grab prediction as one-dimensional tensor\n",
    "        total += labels.size(0)\n",
    "        correct += (prediction == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    \n",
    "    return net, train_loss, train_acc  # net is returned to be fed to the test function later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, criterion, args):\n",
    "    '''\n",
    "    Returns validation loss and accuracy\n",
    "    \n",
    "        Parameters:\n",
    "            net (CNN): a convolutional neural network to validate\n",
    "            criterion (loss function): a loss function to evaluate the model on\n",
    "            args (ArgumentParser): hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            val_loss (float): validation loss\n",
    "            val_acc (float): validation accuracy\n",
    "    '''\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=args.test_batch, shuffle=True)\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (prediction == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, args):\n",
    "    '''\n",
    "    Returns test accuracy\n",
    "    \n",
    "        Parameters:\n",
    "            net (CNN): a trained model\n",
    "            args (ArgumentParser): hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            test_acc (float): test accuracy of a trained model\n",
    "    '''\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.test_batch, shuffle=True)\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(args):\n",
    "    '''\n",
    "    Execute train and validate functions epoch-times to train a CNN model.\n",
    "    Each time, store train & validation loss and accuracy.\n",
    "    Then, test the model and return the result.\n",
    "    \n",
    "        Parameter:\n",
    "            args (ArgumentParser): hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            vars(args) (Dictionary): settings of the model\n",
    "            results (OrderedDict): stored stats of each epoch + test accuracy\n",
    "    '''\n",
    "    net = CNN(model_code = args.model_code,\n",
    "              in_channels = args.in_channels, \n",
    "              out_dim = args.out_dim, \n",
    "              act = args.act, \n",
    "              use_bn = args.use_bn, \n",
    "              dropout = args.dropout\n",
    "             )\n",
    "    net = net.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # select an optimizer\n",
    "    if args.optim == 'adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr)  # learning rate\n",
    "    elif args.optim == 'sgd':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError('Invalid optimizer')\n",
    "\n",
    "    # containers to keep track of statistics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    time_total = 0\n",
    "        \n",
    "    for epoch in range(args.epoch):  # number of training to be completed\n",
    "        time_start = time.time()\n",
    "        net, train_loss, train_acc = train(net, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, criterion, args)\n",
    "        time_end = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        time_duration = round(time_end - time_start, 2)\n",
    "        time_total += time_duration\n",
    "        \n",
    "        # print results of each iteration\n",
    "        print(f'Epoch {epoch+1}, Accuracy(train, validation):{round(train_acc, 2), round(val_acc, 2)}, '\n",
    "              f'Loss(train, validation):{round(train_loss, 4), round(val_loss, 4)}, Time: {time_duration}s')\n",
    "\n",
    "    test_acc = test(net, args)\n",
    "\n",
    "    results = OrderedDict()\n",
    "    results['train_losses'] = [round(x, 4) for x in train_losses]\n",
    "    results['val_losses'] = [round(x, 4) for x in val_losses]\n",
    "    results['train_accs'] = [round(x, 2) for x in train_accs]\n",
    "    results['val_accs'] = [round(x, 2) for x in val_accs]\n",
    "    results['train_acc'] = round(train_acc, 2)\n",
    "    results['val_acc'] = round(val_acc, 2)\n",
    "    results['test_acc'] = round(test_acc, 2)\n",
    "    results['time_total'] = round(time_total, 2)\n",
    "    \n",
    "    return vars(args), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 90240, validation set size: 22560\n",
      "Namespace(act='relu', dropout=0.3, epoch=10, in_channels=1, lr=0.001, model_code='model_2', optim='sgd', out_dim=47, test_batch=256, train_batch=256, use_bn=True)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#### Model Capacity ####\n",
    "args.model_code = 'model_2'\n",
    "args.in_channels = 1\n",
    "args.out_dim = 47\n",
    "args.act = 'relu'\n",
    "\n",
    "#### Regularization ####\n",
    "args.dropout = 0.3\n",
    "args.use_bn = True\n",
    "\n",
    "#### Optimization ####\n",
    "args.optim = 'sgd'\n",
    "args.lr = 0.001  # learning rate\n",
    "args.epoch = 10\n",
    "args.train_batch = 256\n",
    "args.test_batch = 256\n",
    "\n",
    "#### Experimental Variables ####\n",
    "models = ['model_1', 'model_2', 'model_3']\n",
    "optims = ['adam', 'sgd']\n",
    "split_sizes = [0.7, 0.8, 0.85]\n",
    "\n",
    "entire_trainset = torch.utils.data.DataLoader(train_set, shuffle=True)\n",
    "split_train_size = int(0.8*(len(entire_trainset)))  # use 80% as train set\n",
    "split_valid_size = len(entire_trainset) - split_train_size  # use 20% as validation set\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(train_set, [split_train_size, split_valid_size]) \n",
    "print(f'train set size: {split_train_size}, validation set size: {split_valid_size}')\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be focusing on the impact of structures and optimizers to a model first as I believe they're the hyperparmeters with the\n",
    "biggest influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model code: model_1 with adam optimizer\n",
      "Epoch 1, Accuracy(train, validation):(77.89, 86.23), Loss(train, validation):(0.7144, 0.3755), Time: 16.93s\n",
      "Epoch 2, Accuracy(train, validation):(86.17, 87.09), Loss(train, validation):(0.3871, 0.3483), Time: 16.24s\n",
      "Epoch 3, Accuracy(train, validation):(87.61, 88.16), Loss(train, validation):(0.3419, 0.3223), Time: 16.39s\n",
      "Epoch 4, Accuracy(train, validation):(88.41, 88.15), Loss(train, validation):(0.313, 0.3167), Time: 16.37s\n",
      "Epoch 5, Accuracy(train, validation):(89.08, 88.67), Loss(train, validation):(0.291, 0.3097), Time: 16.01s\n",
      "Epoch 6, Accuracy(train, validation):(89.53, 89.0), Loss(train, validation):(0.2758, 0.3017), Time: 15.88s\n",
      "Epoch 7, Accuracy(train, validation):(89.91, 89.18), Loss(train, validation):(0.2617, 0.2938), Time: 15.91s\n",
      "Epoch 8, Accuracy(train, validation):(90.2, 89.14), Loss(train, validation):(0.2511, 0.3038), Time: 16.49s\n",
      "Epoch 9, Accuracy(train, validation):(90.71, 88.82), Loss(train, validation):(0.239, 0.3028), Time: 16.43s\n",
      "Epoch 10, Accuracy(train, validation):(90.94, 89.33), Loss(train, validation):(0.2294, 0.3013), Time: 16.11s\n",
      "{'model_code': 'model_1', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'adam', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 89.1\n",
      "Total time duration: 162.76\n",
      "\n",
      "model code: model_1 with sgd optimizer\n",
      "Epoch 1, Accuracy(train, validation):(35.59, 69.74), Loss(train, validation):(2.7421, 1.2883), Time: 16.4s\n",
      "Epoch 2, Accuracy(train, validation):(72.47, 79.82), Loss(train, validation):(0.9953, 0.6769), Time: 16.41s\n",
      "Epoch 3, Accuracy(train, validation):(78.92, 82.71), Loss(train, validation):(0.6835, 0.5429), Time: 16.43s\n",
      "Epoch 4, Accuracy(train, validation):(81.5, 84.27), Loss(train, validation):(0.5738, 0.4684), Time: 16.42s\n",
      "Epoch 5, Accuracy(train, validation):(82.99, 85.44), Loss(train, validation):(0.516, 0.4344), Time: 16.41s\n",
      "Epoch 6, Accuracy(train, validation):(84.03, 85.58), Loss(train, validation):(0.4782, 0.4214), Time: 16.38s\n",
      "Epoch 7, Accuracy(train, validation):(84.78, 86.21), Loss(train, validation):(0.4517, 0.3908), Time: 16.33s\n",
      "Epoch 8, Accuracy(train, validation):(85.42, 86.76), Loss(train, validation):(0.4321, 0.3757), Time: 16.35s\n",
      "Epoch 9, Accuracy(train, validation):(85.87, 87.23), Loss(train, validation):(0.415, 0.3643), Time: 16.43s\n",
      "Epoch 10, Accuracy(train, validation):(86.24, 87.34), Loss(train, validation):(0.4029, 0.36), Time: 16.33s\n",
      "{'model_code': 'model_1', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'sgd', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 87.02\n",
      "Total time duration: 163.89\n",
      "\n",
      "model code: model_2 with adam optimizer\n",
      "Epoch 1, Accuracy(train, validation):(79.31, 86.66), Loss(train, validation):(0.6443, 0.3698), Time: 26.77s\n",
      "Epoch 2, Accuracy(train, validation):(87.28, 88.61), Loss(train, validation):(0.3463, 0.312), Time: 26.68s\n",
      "Epoch 3, Accuracy(train, validation):(88.44, 88.25), Loss(train, validation):(0.3111, 0.3215), Time: 26.55s\n",
      "Epoch 4, Accuracy(train, validation):(89.05, 88.53), Loss(train, validation):(0.2871, 0.3106), Time: 27.41s\n",
      "Epoch 5, Accuracy(train, validation):(89.87, 88.74), Loss(train, validation):(0.2642, 0.3149), Time: 26.78s\n",
      "Epoch 6, Accuracy(train, validation):(90.16, 89.41), Loss(train, validation):(0.2537, 0.2887), Time: 26.6s\n",
      "Epoch 7, Accuracy(train, validation):(90.65, 89.29), Loss(train, validation):(0.2411, 0.3039), Time: 26.47s\n",
      "Epoch 8, Accuracy(train, validation):(90.91, 88.82), Loss(train, validation):(0.2279, 0.3066), Time: 26.36s\n",
      "Epoch 9, Accuracy(train, validation):(91.19, 89.57), Loss(train, validation):(0.2212, 0.2892), Time: 26.31s\n",
      "Epoch 10, Accuracy(train, validation):(91.47, 89.55), Loss(train, validation):(0.2126, 0.3009), Time: 26.36s\n",
      "{'model_code': 'model_2', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'adam', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 89.33\n",
      "Total time duration: 266.29\n",
      "\n",
      "model code: model_2 with sgd optimizer\n",
      "Epoch 1, Accuracy(train, validation):(53.63, 81.59), Loss(train, validation):(1.8894, 0.5974), Time: 25.97s\n",
      "Epoch 2, Accuracy(train, validation):(82.25, 85.43), Loss(train, validation):(0.5599, 0.4301), Time: 26.3s\n",
      "Epoch 3, Accuracy(train, validation):(85.22, 86.98), Loss(train, validation):(0.4397, 0.3728), Time: 25.77s\n",
      "Epoch 4, Accuracy(train, validation):(86.48, 87.75), Loss(train, validation):(0.3914, 0.3446), Time: 25.74s\n",
      "Epoch 5, Accuracy(train, validation):(87.25, 87.78), Loss(train, validation):(0.3624, 0.3361), Time: 25.94s\n",
      "Epoch 6, Accuracy(train, validation):(87.89, 88.11), Loss(train, validation):(0.3406, 0.3255), Time: 26.42s\n",
      "Epoch 7, Accuracy(train, validation):(88.42, 88.65), Loss(train, validation):(0.325, 0.3141), Time: 26.43s\n",
      "Epoch 8, Accuracy(train, validation):(88.61, 88.54), Loss(train, validation):(0.3125, 0.3063), Time: 26.49s\n",
      "Epoch 9, Accuracy(train, validation):(88.97, 88.86), Loss(train, validation):(0.3018, 0.3022), Time: 26.56s\n",
      "Epoch 10, Accuracy(train, validation):(89.26, 88.77), Loss(train, validation):(0.2921, 0.3077), Time: 26.46s\n",
      "{'model_code': 'model_2', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'sgd', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 88.68\n",
      "Total time duration: 262.08\n",
      "\n",
      "model code: model_3 with adam optimizer\n",
      "Epoch 1, Accuracy(train, validation):(73.89, 84.38), Loss(train, validation):(0.8179, 0.4509), Time: 36.65s\n",
      "Epoch 2, Accuracy(train, validation):(86.54, 85.74), Loss(train, validation):(0.3765, 0.3736), Time: 36.55s\n",
      "Epoch 3, Accuracy(train, validation):(87.91, 87.94), Loss(train, validation):(0.3299, 0.3329), Time: 37.03s\n",
      "Epoch 4, Accuracy(train, validation):(88.67, 88.27), Loss(train, validation):(0.3039, 0.316), Time: 37.66s\n",
      "Epoch 5, Accuracy(train, validation):(89.22, 88.44), Loss(train, validation):(0.2871, 0.3182), Time: 38.14s\n",
      "Epoch 6, Accuracy(train, validation):(89.63, 88.93), Loss(train, validation):(0.2706, 0.308), Time: 36.23s\n",
      "Epoch 7, Accuracy(train, validation):(90.04, 89.34), Loss(train, validation):(0.2607, 0.3), Time: 36.36s\n",
      "Epoch 8, Accuracy(train, validation):(90.29, 88.67), Loss(train, validation):(0.2533, 0.3176), Time: 36.33s\n",
      "Epoch 9, Accuracy(train, validation):(90.6, 89.5), Loss(train, validation):(0.243, 0.2891), Time: 36.64s\n",
      "Epoch 10, Accuracy(train, validation):(90.83, 89.75), Loss(train, validation):(0.2332, 0.2875), Time: 37.46s\n",
      "{'model_code': 'model_3', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'adam', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 89.46\n",
      "Total time duration: 369.05\n",
      "\n",
      "model code: model_3 with sgd optimizer\n",
      "Epoch 1, Accuracy(train, validation):(56.35, 83.63), Loss(train, validation):(1.7213, 0.4984), Time: 36.52s\n",
      "Epoch 2, Accuracy(train, validation):(84.35, 85.71), Loss(train, validation):(0.4717, 0.4161), Time: 36.49s\n",
      "Epoch 3, Accuracy(train, validation):(86.69, 87.27), Loss(train, validation):(0.3826, 0.3524), Time: 36.53s\n",
      "Epoch 4, Accuracy(train, validation):(87.64, 88.22), Loss(train, validation):(0.3444, 0.3282), Time: 36.65s\n",
      "Epoch 5, Accuracy(train, validation):(88.42, 88.41), Loss(train, validation):(0.3202, 0.3129), Time: 36.5s\n",
      "Epoch 6, Accuracy(train, validation):(89.0, 88.85), Loss(train, validation):(0.301, 0.3045), Time: 36.51s\n",
      "Epoch 7, Accuracy(train, validation):(89.38, 88.39), Loss(train, validation):(0.2883, 0.3145), Time: 36.5s\n",
      "Epoch 8, Accuracy(train, validation):(89.77, 88.96), Loss(train, validation):(0.2759, 0.2941), Time: 36.51s\n",
      "Epoch 9, Accuracy(train, validation):(90.11, 89.2), Loss(train, validation):(0.2648, 0.2983), Time: 36.5s\n",
      "Epoch 10, Accuracy(train, validation):(90.36, 89.19), Loss(train, validation):(0.2565, 0.2972), Time: 36.51s\n",
      "{'model_code': 'model_3', 'in_channels': 1, 'out_dim': 47, 'act': 'relu', 'dropout': 0.3, 'use_bn': True, 'optim': 'sgd', 'lr': 0.001, 'epoch': 10, 'train_batch': 256, 'test_batch': 256}\n",
      "Test Accuracy: 88.94\n",
      "Total time duration: 365.22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for opt in optims:\n",
    "        print(f'model code: {model} with {opt} optimizer')\n",
    "        args.model_code = model\n",
    "        args.optim = opt\n",
    "        setting, results = experiment(args)\n",
    "        print(setting)\n",
    "        print('Test Accuracy: {}'.format(results['test_acc']))\n",
    "        print('Total time duration: {}'.format(results['time_total']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "As neural nets became more complex with more nodes:\n",
    "   * execution time got extended significantly\n",
    "   * train loss and validation loss did not change with the adam optimizer\n",
    "       * surprisingly, model_1 with the adam optimizer yielded the best result among all models under the same condition.\n",
    "   * train loss and validation loss improved gradually with the SGD optimizer\n",
    "   \n",
    "Side Notes\n",
    "   * adam optimizer had tendency to overfit data in all models\n",
    "   \n",
    "   \n",
    "## Conclusion\n",
    "* After certain number of convolutional layers, the difference in performance between models is quite insignificant; however, the execution time tends to always increase with the depth, causing unwanted inefficiency. Therefore, the complexity of a convolutional neural network does not justify how efficient it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later: Model of choice to test hyperparameter tuning: model_2 with SGD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
